
## Que es la estadistica descriptiva?

La estadística descriptiva es una rama de la estadística que se encarga de recolectar, organizar, analizar y presentar datos de manera resumida. 
Su objetivo principal es describir y comprender las características básicas de un conjunto de datos a través de medidas cuantitativas y representaciones visuales.

* Algunas de las herramientas y técnicas más comunes de la estadística descriptiva incluyen:

  - Medidas de tendencia central 
  - Medidas de dispercion 
  - Medidas de forma
  - Representaciones visuales


## ¿Qué significa "mentir con la estadística"?

Mentir con la estadística se refiere a la utilización de técnicas estadísticas o la presentación de datos de manera que se manipulen las percepciones, 
se distorsionen los hechos, o se engañe deliberadamente al público. Aunque la estadística es una herramienta poderosa para analizar datos y sacar conclusiones, 
su mal uso puede llevar a conclusiones incorrectas o engañosas.

* Ejemplos comunes de cómo se puede "mentir con la estadística":

  - Seleccion sesgada de datos
  - Manipulacion de escalas en graficos
  - Promedios engañoso 
  - Muestra no representativa 
  - Confundir correlacion con causalidad
  - uso de porcentajes confusos
  - omision de variabilidad

Es importante entender cómo se puede "mentir con la estadística" porque nos permite ser consumidores críticos de la información. 
En un mundo donde los datos y las estadísticas se utilizan para apoyar decisiones en política, salud, economía, y otros campos, 
ser consciente de estas manipulaciones puede ayudarnos a cuestionar la validez de los argumentos presentados y a buscar una comprensión más profunda de la realidad.


## Que es un flujo de trabajo en data science? 

Un flujo de trabajo en Data Science se refiere al conjunto de pasos o procesos que un científico de datos sigue para desarrollar un proyecto de análisis de datos, 
desde la formulación de un problema hasta la implementación de un modelo predictivo y la comunicación de los resultados. 
Aunque los detalles específicos pueden variar según el proyecto, el flujo de trabajo típico en Data Science generalmente incluye las siguientes etapas:

- Definicion del problema 
- Recoleccion de datos
- Limpieza y preparacion de los datos
  * Tareas comunes:
    - Manejo de valores faltantes.
    - Eliminación de duplicados.
    - Normalización y escalado de variables.
    - Transformaciones de características (por ejemplo, convertir fechas en variables numéricas).

- Análisis Exploratorio de Datos (EDA)
  * Objetivo: Explorar los datos para descubrir patrones, detectar anomalías, probar hipótesis, y entender mejor la estructura de los datos.
  * Herramientas comunes: Gráficos, estadísticas descriptivas, y análisis de correlación.

- Selección y Construcción de Modelos
- Evaluación del Modelo
- Implementación
- Comunicación y Visualización de Resultados
- Monitoreo y Mantenimiento

__Flujo de Trabajo Resumido:__  Definición del Problema → Recolección de Datos → Limpieza y Preparación de Datos → Análisis Exploratorio de Datos → Selección y Construcción de Modelos → Evaluación del Modelo → Implementación → Comunicación y Visualización de Resultados → Monitoreo y Mantenimiento